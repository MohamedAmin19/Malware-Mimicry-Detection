{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966791bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice as np_choice\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from IPython.display import display\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import statistics\n",
    "from numpy import inf\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from ge.classify import read_node_label, Classifier\n",
    "from ge import Node2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "warnings.filterwarnings(action = 'ignore')  \n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import pefile\n",
    "from tkinter import *\n",
    "from tkinter.filedialog import askopenfilename\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0135bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the dataset\n",
    "class document_processing(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self,dataset):\n",
    "        self.data=pd.read_csv(dataset)\n",
    "        self.corpus=None\n",
    "        \n",
    "      \n",
    "    def readDataset(self):\n",
    "        \n",
    "       \n",
    "        self.corpus=self.data['ImportedSymbols']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a115aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the TFIDF(Term Frequency Inverse Document Frequency) values for our datasets\n",
    "class TFIDF(document_processing):\n",
    "    \n",
    "    def __init__(self,dataset):\n",
    "        super().__init__(dataset)\n",
    "        super().readDataset()\n",
    "        self.vectorizer = None\n",
    "        self.vocab=None\n",
    "        self.feature_names=None\n",
    "        self.df_list=[]\n",
    "        self.tfidf_vector=None\n",
    "        self.vectorizing_data()\n",
    "        \n",
    "    def vectorizing_data(self):\n",
    "    #Calculating the Bi-Grams TFIDF value for our dataset\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(ngram_range=(2, 2),token_pattern='[^ ()]+')\n",
    "        self.vectorizer = TfidfVectorizer(token_pattern='[^ ()]+')\n",
    "        self.tfidf_vector = self.vectorizer.fit_transform(self.corpus)\n",
    "    #vocab list\n",
    "        vectorizer2 = TfidfVectorizer(token_pattern='[^ ()]+',min_df=0.0005)\n",
    "     \n",
    "        Y=vectorizer2.fit_transform(self.corpus)\n",
    "        self.vocab=vectorizer2.get_feature_names()\n",
    "        \n",
    "        self.feature_names=self.vectorizer.get_feature_names()\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a331445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the w2v(Word2Vec) values for our datasets\n",
    "class Word2VEC(document_processing):\n",
    "\n",
    "    def __init__(self,dataset):\n",
    "        super().__init__(dataset)\n",
    "        super().readDataset()\n",
    "        self.data=[]\n",
    "        self.vector=None\n",
    "        self.vocabulary=None\n",
    "        self.word2vec_vector=None\n",
    "        self.word2vec_processing()\n",
    "        \n",
    "        \n",
    "    def word2vec_processing(self):\n",
    "        for j in range (len(self.corpus)):\n",
    "            temp=[]\n",
    "            new_test=self.corpus[j].split()\n",
    "            for i in range(len(new_test)):\n",
    "                temp.append(new_test[i].lower())\n",
    "                   # print(temp)\n",
    "            self.data.append(temp)    \n",
    "        \n",
    "    \n",
    "        self.word2vec_vector = Word2Vec(self.data,vector_size=300, min_count = 1, \n",
    "                               window = 50)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f88c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create TFIDF and Word2Vec matrices in order to be inserted into the ACO(Ant Colony Optimzation Algorithm)\n",
    "class Model:\n",
    "\n",
    "   \n",
    "    def __init__(self,tfidf_instance=None,word2vec_instance=None):\n",
    "        self.our_tfidf_matrix =[]\n",
    "        self.our_word2vec_matrix =[]\n",
    "        self.tfidf_instance=tfidf_instance\n",
    "        self.word2vec_instance=word2vec_instance\n",
    "        self.tfidf_map={}\n",
    "        \n",
    "    \n",
    "    \n",
    "    def create_tfidf_Matrix(self):\n",
    "        for word in self.tfidf_instance.vocab:\n",
    "            temp_list=[]\n",
    "            for i in range(len(self.tfidf_instance.vocab)):\n",
    "                temp=word+\" \"+self.tfidf_instance.vocab[i]\n",
    "              \n",
    "\n",
    "                if(word==self.tfidf_instance.vocab[i]):\n",
    "                    temp_list.append(0)\n",
    "\n",
    "\n",
    "                elif temp in self.tfidf_instance.vectorizer.vocabulary_.keys():\n",
    "\n",
    "                    c = self.tfidf_instance.vectorizer.vocabulary_[temp]\n",
    "\n",
    "                    y=self.tfidf_instance.tfidf_vector[:,c].todense()\n",
    "                    temp_list.append(y.sum()/len(y))\n",
    "\n",
    "                else:\n",
    "                    temp_list.append(0)\n",
    "\n",
    "\n",
    "            self.our_tfidf_matrix.append(temp_list)\n",
    "        #print(self.our_tfidf_matrix)\n",
    "            \n",
    "    def create_word2vec_Matrix(self):\n",
    "        for word in self.word2vec_instance.vocabulary:\n",
    "            temp_list=[]\n",
    "            for i in range(len(self.word2vec_instance.vocabulary)):\n",
    "                \n",
    "                frequency= self.word2vec_instance.word2vec_vector.wv.similarity(word, self.word2vec_instance.vocabulary[i])\n",
    "                if(word==self.word2vec_instance.vocabulary[i]):\n",
    "            \n",
    "                    temp_list.append(0)\n",
    "                elif(frequency<=0):\n",
    "                    temp_list.append(0)\n",
    "          \n",
    "                else:\n",
    "                    temp_list.append(frequency.astype(np.float64))\n",
    "            self.our_word2vec_matrix.append(temp_list)\n",
    "        #print(self.our_word2vec_matrix)\n",
    "#\n",
    "            \n",
    "    def write_Matrix(self,matrix,text_file):\n",
    "            \n",
    "        with open(text_file, 'w') as f:\n",
    "            f.write(json.dumps(matrix))\n",
    "                \n",
    "    def read_tfidf_Matrix(self,text_file):\n",
    "        with open(text_file, 'r') as f:\n",
    "            self.our_tfidf_matrix = json.loads(f.read())\n",
    "    def read_word2vec_Matrix(self,text_file):\n",
    "         with open(text_file, 'r') as f:\n",
    "            self.our_word2vec_matrix = json.loads(f.read())\n",
    "    def mapping(self):\n",
    "        \n",
    "        for i in range(len(self.tfidf_instance.vocab)):\n",
    "            for j in range(len(self.tfidf_instance.vocab)):\n",
    "                self.tfidf_map[(self.tfidf_instance.vocab[i],self.tfidf_instance.vocab[j])]=self.our_tfidf_matrix[i][j]\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f095fff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tfidf=TFIDF(\"goodware.csv\")\n",
    "w2v=Word2VEC(\"goodware.csv\")\n",
    "model=Model(tfidf_instance=tfidf,word2vec_instance=w2v)\n",
    "model.create_word2vec_Matrix()\n",
    "model.create_tfidf_Matrix()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b422fbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2gramtest_goodware_vectorizer']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saves the TFIDF vectorizer\n",
    "joblib.dump(tfidf.vectorizer,\"2gramtest_goodware_vectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4c1bfa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "w2v2=Word2VEC(\"braz-malware.csv\")\n",
    "tfidf2=TFIDF(\"braz-malware.csv\")\n",
    "model2=Model(tfidf_instance=tfidf2,word2vec_instance=w2v2)\n",
    "model2.create_word2vec_Matrix()\n",
    "model2.create_tfidf_Matrix()\n",
    "#Saves the variables for our application\n",
    "joblib.dump(tfidf2.vectorizer,\"malware_max_vectorizer\")\n",
    "joblib.dump(tfidf2.tfidf_vector,\"malware_vector\")\n",
    "joblib.dump(w2v2.word2vec_vector,\"malware_word2vec\")\n",
    "joblib.dump(w2v.word2vec_vector,\"goodware_word2vec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a399e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the median for our data\n",
    "def findMedian(matrix):\n",
    "    a = matrix\n",
    "    res = np.sum(a, axis=1)\n",
    "    start_points=[]\n",
    "    median=statistics.median(res)\n",
    "\n",
    "    for i in range(len(res)):\n",
    "        if(res[i]>=median):\n",
    "            start_points.append(i)\n",
    "    return start_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88af474",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_points=findMedian(model.our_tfidf_matrix)\n",
    "start_points_2=findMedian(model2.our_tfidf_matrix)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "369bfa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserts the TFIDF and W2V matrices into ACO , in order to find the most relevant API calls \n",
    "class AntColony(object):\n",
    "\n",
    "    def __init__(self, distances, n_ants, n_best, n_iterations, decay,start_points, alpha=1, beta=1):\n",
    "      \n",
    "\n",
    "        self.distances  = distances\n",
    "        self.pheromone = np.ones(self.distances.shape) / len(distances)\n",
    "        self.all_inds = range(len(distances))\n",
    "        self.n_ants = n_ants\n",
    "        self.n_best = n_best\n",
    "        self.n_iterations = n_iterations\n",
    "        self.decay = decay\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.matrix=np.zeros((len(distances),len(distances)))\n",
    "        self.start_points=start_points\n",
    "    #Starts the ants' movements.\n",
    "    def run(self):\n",
    "        #intialize shortest path by None\n",
    "        shortest_path = None\n",
    "        all_time_shortest_path = (\"placeholder\", np.inf)\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            all_paths = self.gen_all_paths(i)\n",
    "            self.spread_pheronome(all_paths, self.n_best, shortest_path=shortest_path)\n",
    "            shortest_path = min(all_paths, key=lambda x: x[1])\n",
    "            print (shortest_path)\n",
    "            if shortest_path[1] < all_time_shortest_path[1]:\n",
    "                all_time_shortest_path = shortest_path            \n",
    "            self.pheromone = self.pheromone * self.decay\n",
    "        \n",
    "        return all_time_shortest_path\n",
    "      \n",
    "        \n",
    "    #spread the pheromones according to the visited paths\n",
    "    def spread_pheronome(self, all_paths, n_best, shortest_path):\n",
    "        sorted_paths = sorted(all_paths, key=lambda x: x[1])\n",
    "        for path, dist in sorted_paths[:n_best]:\n",
    "            for move in path:\n",
    "                \n",
    "                self.pheromone[move] += 1.0 / self.distances[move]\n",
    "\n",
    "    def gen_path_dist(self, path):\n",
    "        total_dist = 0\n",
    "        for ele in path:\n",
    "            total_dist += self.distances[ele]\n",
    "        return total_dist\n",
    "    #start the ant tour using the starting points (points above the median of the row)\n",
    "    def gen_all_paths(self,no):\n",
    "        all_paths = []\n",
    "        for i in range(self.n_ants):\n",
    "            #starting=random.randint(0,len(self.distances)-1)\n",
    "            starting=self.start_points[no]\n",
    "            path = self.gen_path(starting)\n",
    "           \n",
    "            all_paths.append((path, self.gen_path_dist(path)))\n",
    "            \n",
    "        return all_paths\n",
    "    #generate the move that the ant should make according to the the pick_move function and exit if no path found\n",
    "    def gen_path(self, start):\n",
    "        path = []\n",
    "        visited = set()\n",
    "        visited.add(start)\n",
    "        prev = start\n",
    "        for i in range(len(self.distances) - 1):\n",
    "            move=self.pick_move(self.pheromone[prev], self.distances[prev], visited)\n",
    "            if(move==\"Exit\"):\n",
    "                break\n",
    "            else:\n",
    "                path.append((prev, move))\n",
    "                self.matrix[prev,move]=self.matrix[prev,move]+self.distances[prev,move] \n",
    "                prev = move\n",
    "                visited.add(move)\n",
    "        if(move==\"Exit\"):\n",
    "            return path\n",
    "        else:\n",
    "            \n",
    "            path.append((prev, start)) # going back to where we started    \n",
    "            return path\n",
    "    #choose which move to make according to the pheromones and distances \n",
    "    def pick_move(self, pheromone, dist, visited):\n",
    "        pheromone = np.copy(pheromone)\n",
    "        pheromone[list(visited)] = 0\n",
    "        row = pheromone ** self.alpha * (( 1.0 / dist) ** self.beta)\n",
    "        row[row == inf] = 0\n",
    "        row=np.nan_to_num(row, nan=0.0)\n",
    "        norm_row = row / row.sum()\n",
    "        norm_row=np.nan_to_num(norm_row, nan=0.0)\n",
    "\n",
    "       \n",
    "        if(np.all((norm_row == 0))):\n",
    "            return \"Exit\"\n",
    "            \n",
    "           \n",
    "    \n",
    "       \n",
    "        else:\n",
    "            move = np_choice(self.all_inds,1, p=norm_row)[0]\n",
    "            return move\n",
    "        \n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "639f44df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17)], 309.0)\n",
      "([(9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 8)], 308.0)\n",
      "([(10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 8), (8, 9)], 308.0)\n",
      "([(11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 8), (8, 9), (9, 10)], 308.0)\n",
      "([(12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 8), (8, 9), (9, 10), (10, 11)], 309.0)\n",
      "([(13, 14), (14, 15), (15, 16), (16, 17), (17, 8), (8, 9), (9, 10), (10, 11), (11, 12)], 309.0)\n",
      "([(14, 15), (15, 16), (16, 17), (17, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13)], 309.0)\n",
      "([(15, 16), (16, 17), (17, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14)], 309.0)\n",
      "([(16, 17), (17, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15)], 309.0)\n",
      "([(17, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16)], 309.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "ant_colony = AntColony(np.array(model.our_tfidf_matrix),1, 1, len(start_points), 0.95,start_points ,alpha=1, beta=1) \n",
    "shortest_path = ant_colony.run()\n",
    "# print (\"shorted_path: {}\".format(shortest_path))\n",
    "\n",
    "\n",
    "# with open('goodware_antcolony_results.txt', 'w') as f:\n",
    "#     f.write(json.dumps(ant_colony.matrix.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d1f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_colony2 = AntColony(np.array(model2.our_tfidf_matrix),1, 1, len(start_points2), 0.95,start_points2, alpha=1, beta=1,) \n",
    "shortest_path2 = ant_colony2.run()\n",
    "print (\"shorted_path: {}\".format(shortest_path2))\n",
    "with open('malware_antcolony_results.txt', 'w') as f:\n",
    "    f.write(json.dumps(ant_colony2.matrix.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7bb25665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Graph():\n",
    "    #Create a graph to visualize the ant colony matrix\n",
    "    def __init__(self,save_location,graph=None):\n",
    "        self.graph=graph\n",
    "        self.save_location=save_location\n",
    "       \n",
    "    def importMatrix(self,graph_location):\n",
    "        \n",
    "        with open(graph_location, 'r') as f:\n",
    "            self.graph = json.loads(f.read())\n",
    "            \n",
    "        self.graph=np.array(self.graph)\n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def drawGraph(self):\n",
    "        \n",
    "        Z = nx.DiGraph(self.graph)\n",
    "        labels = nx.get_edge_attributes(Z,'weight')\n",
    "        G = nx.DiGraph()                                                                                                                                     \n",
    "        fedges = filter(lambda x: Z.degree()[x[0]] > 0 and Z.degree()[x[1]] > 0,Z.edges())\n",
    "        G.add_edges_from(fedges)\n",
    "        f = plt.figure()\n",
    "        f.set_figwidth(10)\n",
    "        f.set_figheight(10)\n",
    "        pos=nx.spring_layout(G) \n",
    "        nx.draw_networkx(G,pos)\n",
    "\n",
    "        G=nx.draw_networkx_edge_labels(G,pos,edge_labels=labels)\n",
    "      \n",
    "        plt.savefig(self.save_location,bbox_inches='tight')\n",
    "        plt.close()\n",
    "    def saveEdges(self,file_name):\n",
    "        Z = nx.DiGraph(self.graph)\n",
    "#         labels = nx.get_edge_attributes(Z,'weight')\n",
    "#         G = nx.DiGraph()                                                                                                                                     \n",
    "#         fedges = filter(lambda x: Z.degree()[x[0]] > 0 and Z.degree()[x[1]] > 0,Z.edges())\n",
    "#         G.add_edges_from(fedges)\n",
    "        nx.write_edgelist(Z,file_name,data=False)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00eac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_instance2=Graph(\"our_tfidf_malware_graph_adjusted.png\",graph=None)\n",
    "graph_instance2.importMatrix(\"final_tfidf_malware.txt\")\n",
    "# graph_instance2.drawGraph()\n",
    "graph_instance2.saveEdges(\"malware_tfidf_edgelist.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_instance4=Graph(\"tfidf_graph_embedding_malware.png\",graph=None)\n",
    "graph_instance4.importMatrix(\"tfidf_malware_graph_embedding.txt\")\n",
    "graph_instance4.drawGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7febcdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"malware_antcolony_results.txt\", 'r') as f:\n",
    "        ant_mal = json.loads(f.read())\n",
    "with open(\"goodware_antcolony_results.txt\", 'r') as f:\n",
    "        ant_good = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02478798",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(ant_mal,\"malware_ant\")\n",
    "joblib.dump(ant_good,\"goodware_ant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cbb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping():\n",
    "    def f1():\n",
    "    \n",
    "        mix_goodware_map={}\n",
    "        mix_malware_map={}\n",
    "\n",
    "        for i in range(len(model.tfidf_instance.vocab)):\n",
    "\n",
    "            for j in range(len(model.tfidf_instance.vocab)):\n",
    "\n",
    "                mix_goodware_map[(model.tfidf_instance.vocab[i],model.tfidf_instance.vocab[j])]=mix_goodware[i][j]\n",
    "\n",
    "        for i in range(len(model2.tfidf_instance.vocab)):\n",
    "                    for j in range(len(model2.tfidf_instance.vocab)):\n",
    "                        mix_malware_map[(model2.tfidf_instance.vocab[i],model2.tfidf_instance.vocab[j])]=mix_malware[i][j]\n",
    "    def f2():\n",
    "        mix_goodware_map={}\n",
    "        mix_malware_map={}\n",
    "\n",
    "        for i in range(len(model.tfidf_instance.vocab)):\n",
    "\n",
    "            for j in range(len(model.tfidf_instance.vocab)):\n",
    "                if(ant_good[i][j]!=0):\n",
    "\n",
    "                    mix_goodware_map[(model.tfidf_instance.vocab[i],model.tfidf_instance.vocab[j])]=ant_good[i][j]\n",
    "\n",
    "        for i in range(len(model2.tfidf_instance.vocab)):\n",
    "            for j in range(len(model2.tfidf_instance.vocab)):\n",
    "                if(ant_mal[i][j]!=0):\n",
    "\n",
    "                    mix_malware_map[(model2.tfidf_instance.vocab[i],model2.tfidf_instance.vocab[j])]=ant_mal[i][j]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8d84f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['goodware_ant']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(mix_malware_map,\"malware_ant\")\n",
    "joblib.dump(mix_goodware_map,\"goodware_ant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbee769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class exportFeatures:\n",
    "    \n",
    "    #Calculating the number of maximum features in each sequence in the dataset according to TFIDF goodware/malware\n",
    "    def maxTfidf(self,dataset):\n",
    "         \n",
    "        csv_input = pd.read_csv('braz-mal20.csv')\n",
    "        corpus=dataset\n",
    "        predicted_good=[]\n",
    "        predicted_mal=[]\n",
    "        equal=[]\n",
    "        for j in range (len(corpus)):\n",
    "            sum_mal=0\n",
    "            sum_good=0\n",
    "            new_test=corpus[j].split()\n",
    "            \n",
    "\n",
    "\n",
    "            for i in range(len(new_test)-1):\n",
    "                temp=new_test[i].lower()+\" \"+new_test[i+1].lower()\n",
    "               \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "                if(temp in tfidf.vectorizer.vocabulary_.keys()):\n",
    "                    sum_good+=1\n",
    "                    \n",
    "\n",
    "\n",
    "                if(temp in tfidf2.vectorizer.vocabulary_.keys()):\n",
    "                    sum_mal+=1\n",
    "                    \n",
    "               \n",
    "        \n",
    "            predicted_good.append(sum_good)\n",
    "            predicted_mal.append(sum_mal)\n",
    "        csv_input['TFIDF_good'] = predicted_good\n",
    "        csv_input['TFIDF_mal']=predicted_mal\n",
    "        csv_input.to_csv('braz-mal20.csv')\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    def maxAnt(self):\n",
    "       #Caluclating the ant score for each sequeunce in the dataset according to the ACO algorithm\n",
    "        data=pd.read_csv('braz-mal18.csv')\n",
    "        corpus=data['ImportedSymbols']\n",
    "#         corpus2=dataset2\n",
    "        csv_input = pd.read_csv('braz-mal18.csv')\n",
    "\n",
    "        predicted=[]\n",
    "        for j in range (len(corpus)):\n",
    "            sum_mal=0\n",
    "            sum_good=0\n",
    "            new_test=corpus[j].split()\n",
    "\n",
    "\n",
    "            for i in range(len(new_test)-1):\n",
    "\n",
    "\n",
    "                if(mix_malware_map.get((new_test[i].lower(),new_test[i+1].lower()))!=None):\n",
    "                    sum_mal+=1\n",
    "                    \n",
    "\n",
    "\n",
    "                if(mix_goodware_map.get((new_test[i].lower(),new_test[i+1].lower()))!=None):\n",
    "                    sum_good+=1\n",
    "            \n",
    "                    \n",
    "           \n",
    "            if(sum_good>sum_mal):\n",
    "                predicted.append(0)\n",
    "            elif(sum_mal>sum_good):\n",
    "                predicted.append(1)\n",
    "            else:\n",
    "                predicted.append(2)\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        csv_input['ant_score'] = predicted\n",
    "        \n",
    "        csv_input.to_csv('braz-mal20.csv')\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def TFIDF_extraction(self,dataset,dataset2):\n",
    "    #Calculating the sum of TFIDF values for each sequence in the dataset according to the malware and the goodware vectorizer\n",
    "        \n",
    "        data=pd.read_csv('braz-malware.csv')\n",
    "        corpus=data['ImportedSymbols']\n",
    "        corpus2=dataset2\n",
    "        csv_input = pd.read_csv('braz-mal20.csv')\n",
    "\n",
    "\n",
    "        predicted_good=[]\n",
    "        predicted_mal=[]\n",
    "        equal=[]\n",
    "        for j in range (len(corpus)):\n",
    "            sum_mal=0\n",
    "            sum_good=0\n",
    "            new_test=corpus[j].split()\n",
    "            \n",
    "\n",
    "\n",
    "            for i in range(len(new_test)-1):\n",
    "                temp=new_test[i].lower()+\" \"+new_test[i+1].lower()\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "                if(model2.tfidf_map.get((new_test[i].lower(),new_test[i+1].lower()))!=None):\n",
    "                    sum_mal+=model2.tfidf_map.get((new_test[i].lower(),new_test[i+1].lower()))\n",
    "                \n",
    "    \n",
    "\n",
    "                if(model.tfidf_map.get((new_test[i].lower(),new_test[i+1].lower()))!=None):\n",
    "                    sum_good+=model.tfidf_map.get((new_test[i].lower(),new_test[i+1].lower()))\n",
    "                    \n",
    "                    \n",
    "\n",
    "        \n",
    "            \n",
    "            predicted_good.append(sum_good)\n",
    "            predicted_mal.append(sum_mal)\n",
    "            \n",
    "                   \n",
    "        for j in range (len(corpus2)):\n",
    "            sum_mal=0\n",
    "            sum_good=0\n",
    "            new_test=corpus2[j].split()\n",
    "\n",
    "\n",
    "            for i in range(len(new_test)-1):\n",
    "\n",
    "\n",
    "                if(model2.tfidf_map.get((new_test[i].lower(),new_test[i+1].lower()))!=None):\n",
    "                    sum_mal+=model2.tfidf_map.get((new_test[i].lower(),new_test[i+1].lower()))\n",
    "                \n",
    "\n",
    "                   \n",
    "\n",
    "\n",
    "                if(model.tfidf_map.get((new_test[i].lower(),new_test[i+1].lower()))!=None):\n",
    "                    sum_good+=model.tfidf_map.get((new_test[i].lower(),new_test[i+1].lower()))\n",
    "                    \n",
    "                     \n",
    "                    \n",
    "            predicted_good.append(sum_good)\n",
    "            predicted_mal.append(sum_mal)\n",
    "                \n",
    "        \n",
    "            \n",
    "            \n",
    "\n",
    "        csv_input['TFIDF_mal'] = predicted_mal   \n",
    "        csv_input['TFIDF_good'] = predicted_good\n",
    "        csv_input.to_csv('braz-mal20.csv')\n",
    "             \n",
    "    def exW2v(self,dataset):\n",
    "    #Calculating the sum of W2V frequencies for each sequence in the dataset \n",
    "        corpus=dataset\n",
    "\n",
    "        predicted_good=[]\n",
    "        predicted_mal=[]\n",
    "        equal=[]\n",
    "        \n",
    "        for j in range (len(corpus)):\n",
    "            sum_mal=1\n",
    "            sum_good=1\n",
    "            new_test=corpus[j].split()\n",
    "            \n",
    "\n",
    "          \n",
    "            for i in range(len(new_test)-1):\n",
    "                \n",
    "                \n",
    "                if(new_test[i].lower() in w2v.word2vec_vector.wv.key_to_index and new_test[i+1].lower() in w2v.word2vec_vector.wv.key_to_index):\n",
    "                    \n",
    "                    frequency= w2v.word2vec_vector.wv.similarity(new_test[i].lower(),new_test[i+1].lower())\n",
    "#                     if(frequency>0):\n",
    "                        \n",
    "                    sum_good+=frequency\n",
    "                if(new_test[i].lower() in w2v2.word2vec_vector.wv.key_to_index and new_test[i+1].lower()  in w2v2.word2vec_vector.wv.key_to_index):\n",
    "                    frequency2=w2v2.word2vec_vector.wv.similarity(new_test[i].lower(),new_test[i+1].lower())\n",
    "#                     if(frequency2>0):\n",
    "                        \n",
    "                    sum_mal+=frequency2\n",
    "               \n",
    "            \n",
    "            predicted_good.append(sum_good)\n",
    "            predicted_mal.append(sum_mal)\n",
    "                \n",
    "        \n",
    "            \n",
    "            \n",
    "\n",
    "        csv_input['w2v_mal'] = predicted_mal   \n",
    "        csv_input['w2v_good'] = predicted_good\n",
    "        csv_input.to_csv('braz-mal20.csv')\n",
    "             \n",
    "          \n",
    "   \n",
    " \n",
    "    def twogramtest(self,dataset,dataset2):\n",
    "    #Calculating the number of maximum bigrams in each sequence in  the dataset\n",
    "       \n",
    "            data=pd.read_csv('braz-malware.csv')\n",
    "            corpus=data['ImportedSymbols']\n",
    "            corpus2=dataset2\n",
    "            csv_input = pd.read_csv('braz-mal20.csv')\n",
    "\n",
    "            predicted_good=[]\n",
    "            predicted_mal=[]\n",
    "            predicted_grams=[]\n",
    "            equal=[]\n",
    "            for j in range (len(corpus)):\n",
    "                sum_mal=0\n",
    "                sum_good=0\n",
    "                new_test=corpus[j].split()\n",
    "\n",
    "\n",
    "                for i in range(len(new_test)-1):\n",
    "                    temp=new_test[i].lower()+\" \"+new_test[i+1].lower()\n",
    "\n",
    "\n",
    "                    if(temp in tfidf.vectorizer.vocabulary_.keys()):\n",
    "                        sum_good+=1\n",
    "                    \n",
    "\n",
    "\n",
    "                    if(temp in tfidf2.vectorizer.vocabulary_.keys()):\n",
    "                        sum_mal+=1\n",
    "                        \n",
    "                if(sum_mal>sum_good):\n",
    "                    predicted_grams.append(1)\n",
    "                elif(sum_mal==sum_good):\n",
    "                    predicted_grams.append(2)\n",
    "\n",
    "\n",
    "#                 predicted_good.append(sum_good)\n",
    "#                 predicted_mal.append(sum_mal)\n",
    "\n",
    "            for j in range (len(corpus2)):\n",
    "                sum_mal=0\n",
    "                sum_good=0\n",
    "                new_test=corpus2[j].split()\n",
    "\n",
    "\n",
    "                for i in range(len(new_test)-1):\n",
    "\n",
    "\n",
    "                    temp=new_test[i].lower()+\" \"+new_test[i+1].lower()\n",
    "                    if(temp in tfidf.vectorizer.vocabulary_.keys()):\n",
    "                        sum_good+=1\n",
    "                    if(temp in tfidf2.vectorizer.vocabulary_.keys()):\n",
    "                        sum_mal+=1\n",
    "                if(sum_mal<sum_good):\n",
    "                    predicted_grams.append(0)\n",
    "                elif(sum_mal==sum_good):\n",
    "                    predicted_grams.append(2)\n",
    "                \n",
    "#                 predicted_good.append(sum_good)\n",
    "#                 predicted_mal.append(sum_mal)\n",
    "\n",
    "            csv_input['max_bi'] = predicted_grams\n",
    "         \n",
    "            csv_input.to_csv('braz-mal20.csv')\n",
    "            \n",
    "\n",
    "    def twogram(self,dataset,dataset2):\n",
    "        #Calculating the number of bigrams in each sequence in  the dataset\n",
    "\n",
    "            data=pd.read_csv('braz-malware.csv')\n",
    "            corpus=data['ImportedSymbols']\n",
    "            corpus2=dataset2\n",
    "            csv_input = pd.read_csv('braz-mal.csv')\n",
    "\n",
    "            predicted_good=[]\n",
    "            predicted_mal=[]\n",
    "            equal=[]\n",
    "            for j in range (len(corpus)):\n",
    "                sum_mal=0\n",
    "                sum_good=0\n",
    "                new_test=corpus[j].split()\n",
    "\n",
    "\n",
    "                for i in range(len(new_test)-1):\n",
    "                    temp=new_test[i].lower()+\" \"+new_test[i+1].lower()\n",
    "\n",
    "\n",
    "                    if(temp in tfidf.vectorizer.vocabulary_.keys()):\n",
    "                        sum_good+=1\n",
    "\n",
    "\n",
    "\n",
    "                    if(temp in tfidf2.vectorizer.vocabulary_.keys()):\n",
    "                        sum_mal+=1\n",
    "\n",
    "            \n",
    "\n",
    "                predicted_good.append(sum_good)\n",
    "                predicted_mal.append(sum_mal)\n",
    "\n",
    "            for j in range (len(corpus2)):\n",
    "                sum_mal=0\n",
    "                sum_good=0\n",
    "                new_test=corpus2[j].split()\n",
    "\n",
    "\n",
    "                for i in range(len(new_test)-1):\n",
    "\n",
    "\n",
    "                    temp=new_test[i].lower()+\" \"+new_test[i+1].lower()\n",
    "                    if(temp in tfidf.vectorizer.vocabulary_.keys()):\n",
    "                        sum_good+=1\n",
    "                    if(temp in tfidf2.vectorizer.vocabulary_.keys()):\n",
    "                        sum_mal+=1\n",
    "             \n",
    "\n",
    "                predicted_good.append(sum_good)\n",
    "                predicted_mal.append(sum_mal)\n",
    "\n",
    "            csv_input['2gram_good'] = predicted_good\n",
    "            csv_input['2gram_mal']=predicted_mal\n",
    "            csv_input.to_csv('braz-mal20.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "   \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "   \n",
    "        \n",
    "    \n",
    "\n",
    "   \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d10441a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls=exportFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c789fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.onegram(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0e6d6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.twogram(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24a8881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.twogramtest(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cf928",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.maxTfidf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "590ab758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cls.maxAnt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.exTfidf(x)\n",
    "cls.exTfidf(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff4714",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.exW2v(x)\n",
    "cls.exW2v(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
