class Word2VEC(document_processing):
    
    def __init__(self,dataset):
        super().__init__(dataset)
        super().readDataset()
        self.data=[]
        self.vector=None
        self.vocabulary=None
        self.word2vec_vector=None
        self.word2vec_processing()
        
        
    def word2vec_processing(self):
        for it in range(len(self.corpus)):
            for i in sent_tokenize(self.corpus[it]):
                temp = []
                for j in word_tokenize(i):
                    temp.append(j.lower())
                   # print(temp)
                self.data.append(temp)
                
                
        
        print(len(self.data))    
                
        self.vector= TfidfVectorizer(min_df=0.001)
        Y=self.vector.fit_transform(self.corpus)
        self.vocabulary=self.vector.get_feature_names()
        #print(self.vocabulary)
        #print(len(self.vocabulary))
        self.word2vec_vector = Word2Vec(self.data, vector_size=100, min_count = 1, 
                               window = 5)
        
        
    def displayAll(self):
        vocabulary = list(model1.wv.key_to_index)
        for i in vocabulary:
            print("The similiarity for " + i )
            print(model1.wv.most_similar(i))
            
  def median():
       a = np.array(model.our_tfidf_matrix)
       res = np.sum(a, axis=1)
       start_points=[]
       median=statistics.median(res)
       for i in range(len(res)):
          if(res[i]>=median):
            start_points.append(i)
