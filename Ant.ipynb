{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966791bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice as np_choice\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from IPython.display import display\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "import pants\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import statistics\n",
    "from numpy import inf\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "warnings.filterwarnings(action = 'ignore')  "
   ]
   {
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.read_edgelist('goodware_tfidf_edgelist.txt',\n",
    "create_using = nx.DiGraph(), nodetype = None, data = [('weight', int)])\n",
    "\n",
    "model_n2v = Node2Vec(G, walk_length=100, num_walks=80,\n",
    "p=0.25, q=4, workers=1, use_rejection_sampling=0)\n",
    "model_n2v.train(window_size = 20, iter = 3)\n",
    "\n",
    "print(model2_n2v.w2v_model)\n",
    "\n",
    "with open('tfidf_goodware_graph_embedding.txt', 'w') as f:\n",
    "    f.write(json.dumps(embeddings))\n",
    "\n",
    "G=nx.read_edgelist('malware_tfidf_edgelist.txt',\n",
    "create_using = nx.DiGraph(), nodetype = None, data = [('weight', int)])\n",
    "\n",
    "model2_n2v = Node2Vec(G, walk_length=100, num_walks=80,\n",
    "p=0.25, q=4, workers=1, use_rejection_sampling=0)\n",
    "model2_n2v.train(window_size = 20, iter = 3)\n",
    "\n",
    "with open('tfidf_malware_graph_embedding.txt', 'w') as f:\n",
    "    f.write(json.dumps(embeddings2))\n",
    "with open('tfidf_goodware_graph_embedding.txt', 'r') as f:\n",
    "            embeddings = json.loads(f.read())\n",
    "with open('tfidf_malware_graph_embedding.txt', 'r') as f:\n",
    "            embeddings2 = json.loads(f.read())\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "cluster.fit_predict(embeddings)\n",
    "print(cluster.labels_)\n",
    "plt.scatter(np.array(embeddings)[:, 0], np.array(embeddings)[:, 1], c=cluster.labels_, cmap='rainbow')\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n",
    "cluster.fit_predict(embeddings)\n",
    "print(cluster.labels_)\n",
    "plt.scatter(np.array(embeddings)[:, 0], np.array(embeddings)[:, 1], c=cluster.labels_, cmap='rainbow')\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(embeddings)\n",
    "y_kmeans = kmeans.predict(embeddings)\n",
    "f = plt.figure()\n",
    "f.set_figwidth(10)\n",
    "f.set_figheight(10)\n",
    "plt.scatter(np.array(embeddings)[:, 0], np.array(embeddings)[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "# centers = kmeans.cluster_centers_\n",
    "# plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(embeddings)\n",
    "y_kmeans = kmeans.predict(embeddings)\n",
    "f = plt.figure()\n",
    "f.set_figwidth(10)\n",
    "f.set_figheight(10)\n",
    "plt.scatter(np.array(embeddings)[:, 0], np.array(embeddings)[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "# centers = kmeans.cluster_centers_\n",
    "# plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
    "mix_goodware = [[a*b for a,b in zip(x,y)] for x,y in zip(model.our_tfidf_matrix,embeddings)]\n",
    "# with open(\"mix_goodware.txt\", 'w') as f:\n",
    "            \n",
    "#     f.write(json.dumps(mix_goodware))\n",
    "mix_malware=[[a*b for a,b in zip(x,y)] for x,y in zip(model2.our_tfidf_matrix,embeddings2)]\n",
    "# with open(\"mix_malware.txt\", 'w') as f:\n",
    "            \n",
    "#     f.write(json.dumps(mix_malware))\n",
    "with open(\"malware_antcolony_results.txt\", 'r') as f:\n",
    "            ant_mal = json.loads(f.read())\n",
    "with open(\"goodware_antcolony_results.txt\", 'r') as f:\n",
    "            ant_good = json.loads(f.read())\n",
    "mix_goodware_map={}\n",
    "mix_malware_map={}\n",
    "\n",
    "for i in range(len(model.tfidf_instance.vocab)):\n",
    "    \n",
    "    for j in range(len(model.tfidf_instance.vocab)):\n",
    "        \n",
    "        mix_goodware_map[(model.tfidf_instance.vocab[i],model.tfidf_instance.vocab[j])]=mix_goodware[i][j]\n",
    "                \n",
    "for i in range(len(model2.tfidf_instance.vocab)):\n",
    "            for j in range(len(model2.tfidf_instance.vocab)):\n",
    "                mix_malware_map[(model2.tfidf_instance.vocab[i],model2.tfidf_instance.vocab[j])]=mix_malware[i][j]\n",
    "del ant_good\n",
    "del ant_mal\n",
    "data=pd.read_csv(\"brazilian-malware.csv\")\n",
    "corpus=data['ImportedSymbols']\n",
    "data2=pd.read_csv(\"goodware.csv\")\n",
    "corpus2=data2['ImportedSymbols']\n",
    "x=corpus.tolist()\n",
    "y=corpus2.tolist()\n",
    "def remove_common(a, b):\n",
    "  \n",
    "    for i in a:\n",
    "        if i in b:\n",
    "            \n",
    "            a.remove(i)\n",
    "            \n",
    "  \n",
    "#     print(\"list1 : \", a)\n",
    "#     print(\"list2 : \", b)\n",
    "  \n",
    "remove_common(x,y)\n",
    "print(len(corpus))\n",
    "print(len(corpus2))\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "corpus=x\n",
    "predicted_good=[]\n",
    "predicted_mal=[]\n",
    "equal=[]\n",
    "for j in range (len(corpus)):\n",
    "    sum_mal=0\n",
    "    sum_good=0\n",
    "    new_test=corpus[j].split()\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(new_test)-1):\n",
    "\n",
    "\n",
    "                if(mix_malware_map.get((new_test[i].lower(),new_test[i+1].lower()))!=None):\n",
    "                    sum_mal+=1\n",
    "                    \n",
    "\n",
    "\n",
    "                if(mix_goodware_map.get((new_test[i].lower(),new_test[i+1].lower()))!=None):\n",
    "                    sum_good+=1\n",
    "                    \n",
    "          \n",
    "            \n",
    "\n",
    "\n",
    "    if(sum_mal<sum_good):\n",
    "        predicted_good.append(1)\n",
    "\n",
    "\n",
    "\n",
    "    elif(sum_mal>sum_good):\n",
    "        predicted_mal.append(1)\n",
    "\n",
    "    else:\n",
    "        equal.append(1)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Length of the dataset is equal to \" + str(len(corpus)))\n",
    "print(\"Instances classified as goodware \"+str(len(predicted_good)))\n",
    "print(\"Instances classified as malware \" + str(len(predicted_mal)))\n",
    "print(\"Cant Classify = \" +str(len(equal)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0135bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class document_processing(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self,dataset):\n",
    "        self.data=pd.read_csv(dataset)\n",
    "        self.corpus=None\n",
    "        \n",
    "      \n",
    "    def readDataset(self):\n",
    "        \n",
    "       \n",
    "        self.corpus=self.data['ImportedSymbols']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a115aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TFIDF(document_processing):\n",
    "    \n",
    "    def __init__(self,dataset):\n",
    "        super().__init__(dataset)\n",
    "        super().readDataset()\n",
    "        self.vectorizer = None\n",
    "        self.vocab=None\n",
    "        self.feature_names=None\n",
    "        self.df_list=[]\n",
    "        self.tfidf_vector=None\n",
    "        self.vectorizing_data()\n",
    "        \n",
    "    def vectorizing_data(self):\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
    "        self.tfidf_vector = self.vectorizer.fit_transform(self.corpus)\n",
    "        vectorizer2 = TfidfVectorizer(min_df=0.001)\n",
    "        Y=vectorizer2.fit_transform(self.corpus)\n",
    "        self.vocab=vectorizer2.get_feature_names()\n",
    "        self.feature_names=self.vectorizer.get_feature_names()\n",
    "    def display_tfidf(self):\n",
    "        for i in range(5):\n",
    "            first_document_vector=self.tfidf_vector[i]\n",
    "            df = pd.DataFrame(first_document_vector.T.todense(), index=self.feature_names, columns=[\"tfidf\"]) \n",
    "            df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "            df = df[(df.T != 0).any()]\n",
    "            self.df_list.append(df)\n",
    "        for i in self.df_list:\n",
    "\n",
    "\n",
    "            display(i)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a331445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VEC(document_processing):\n",
    "    \n",
    "    def __init__(self,dataset):\n",
    "        super().__init__(dataset)\n",
    "        super().readDataset()\n",
    "        self.data=[]\n",
    "        self.vector=None\n",
    "        self.vocabulary=None\n",
    "        self.word2vec_vector=None\n",
    "        self.word2vec_processing()\n",
    "        \n",
    "        \n",
    "    def word2vec_processing(self):\n",
    "        for it in range(len(self.corpus)):\n",
    "            for i in sent_tokenize(self.corpus[it]):\n",
    "                temp = []\n",
    "                for j in word_tokenize(i):\n",
    "                    temp.append(j.lower())\n",
    "                   # print(temp)\n",
    "                self.data.append(temp)\n",
    "                \n",
    "                \n",
    "        \n",
    "        print(len(self.data))    \n",
    "                \n",
    "        self.vector= TfidfVectorizer(min_df=0.001)\n",
    "        Y=self.vector.fit_transform(self.corpus)\n",
    "        self.vocabulary=self.vector.get_feature_names()\n",
    "        #print(self.vocabulary)\n",
    "        #print(len(self.vocabulary))\n",
    "        self.word2vec_vector = Word2Vec(self.data, vector_size=100, min_count = 1, \n",
    "                               window = 5)\n",
    "        \n",
    "        \n",
    "    def displayAll(self):\n",
    "        vocabulary = list(model1.wv.key_to_index)\n",
    "        for i in vocabulary:\n",
    "            print(\"The similiarity for \" + i )\n",
    "            print(model1.wv.most_similar(i))\n",
    "\n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "   \n",
    "    def __init__(self,tfidf_instance=None,word2vec_instance=None):\n",
    "        self.our_tfidf_matrix =[]\n",
    "        self.our_word2vec_matrix =[]\n",
    "        self.tfidf_instance=tfidf_instance\n",
    "        self.word2vec_instance=word2vec_instance\n",
    "    \n",
    "    \n",
    "    def create_tfidf_Matrix(self):\n",
    "        for word in self.tfidf_instance.vocab:\n",
    "            temp_list=[]\n",
    "            for i in range(len(self.tfidf_instance.vocab)):\n",
    "                temp=word+\" \"+self.tfidf_instance.vocab[i]\n",
    "                temp2=self.tfidf_instance.vocab[i]+\" \"+word\n",
    "\n",
    "                if(word==self.tfidf_instance.vocab[i]):\n",
    "                    temp_list.append(0)\n",
    "\n",
    "\n",
    "                elif temp in self.tfidf_instance.vectorizer.vocabulary_.keys():\n",
    "\n",
    "                    c = self.tfidf_instance.vectorizer.vocabulary_[temp]\n",
    "\n",
    "                    y=self.tfidf_instance.tfidf_vector[:,c].todense()\n",
    "                    temp_list.append(y.sum()/len(y))\n",
    "                elif temp2 in self.tfidf_instance.vectorizer.vocabulary_.keys():\n",
    "                    \n",
    "                    c = self.tfidf_instance.vectorizer.vocabulary_[temp2]\n",
    "                    y=self.tfidf_instance.tfidf_vector[:,c].todense()\n",
    "                    temp_list.append(y.sum()/len(y))\n",
    "\n",
    "                else:\n",
    "                    temp_list.append(0)\n",
    "\n",
    "\n",
    "            self.our_tfidf_matrix.append(temp_list)\n",
    "        #print(self.our_tfidf_matrix)\n",
    "            \n",
    "    def create_word2vec_Matrix(self):\n",
    "        for word in self.word2vec_instance.vocabulary:\n",
    "            temp_list=[]\n",
    "            for i in range(len(self.word2vec_instance.vocabulary)):\n",
    "                \n",
    "                frequency= self.word2vec_instance.word2vec_vector.wv.similarity(word, self.word2vec_instance.vocabulary[i])\n",
    "                if(word==self.word2vec_instance.vocabulary[i]):\n",
    "            \n",
    "                    temp_list.append(0)\n",
    "                elif(frequency<=0):\n",
    "                    temp_list.append(0)\n",
    "          \n",
    "                else:\n",
    "                    temp_list.append(frequency.astype(np.float64))\n",
    "            self.our_word2vec_matrix.append(temp_list)\n",
    "        #print(self.our_word2vec_matrix)\n",
    "#\n",
    "            \n",
    "    def write_Matrix(self,matrix,text_file):\n",
    "            \n",
    "        with open(text_file, 'w') as f:\n",
    "            f.write(json.dumps(matrix))\n",
    "                \n",
    "    def read_tfidf_Matrix(self,text_file):\n",
    "        with open(text_file, 'r') as f:\n",
    "            self.our_tfidf_matrix = json.loads(f.read())\n",
    "    def read_word2vec_Matrix(self,text_file):\n",
    "         with open(text_file, 'r') as f:\n",
    "            self.our_word2vec_matrix = json.loads(f.read())\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# tfidf=TFIDF(\"goodware.csv\")\n",
    "#word2vec=Word2VEC(\"goodware.csv\")\n",
    "#tfidf=\n",
    "# model=Model(tfidf_instance=tfidf)\n",
    "# model.create_tfidf_Matrix()\n",
    "#model=Model(tfidf_instance=tfidf)\n",
    "#model.create_word2vec_Matrix()\n",
    "#model.write_Matrix(model.our_word2vec_matrix,\"our_goodware_word2vec_matrix.txt\")\n",
    "model=Model()\n",
    "model.read_tfidf_Matrix(\"our_malware_tfidf_matrix.txt\")\n",
    "model2=Model()\n",
    "model2.read_tfidf_Matrix(\"our_goodware_tfidf_matrix.txt\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a399e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(model.our_tfidf_matrix)\n",
    "res = np.sum(a, axis=1)\n",
    "start_points=[]\n",
    "median=statistics.median(res)\n",
    "for i in range(len(res)):\n",
    "    if(res[i]>=median):\n",
    "        start_points.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88af474",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array(model2.our_tfidf_matrix)\n",
    "res2 = np.sum(b, axis=1)\n",
    "start_points2=[]\n",
    "median2=statistics.median(res2)\n",
    "for i in range(len(res2)):\n",
    "    if(res2[i]>=median2):\n",
    "        start_points2.append(i)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369bfa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntColony(object):\n",
    "\n",
    "    def __init__(self, distances, n_ants, n_best, n_iterations, decay,start_points, alpha=1, beta=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            distances (2D numpy.array): Square matrix of distances. Diagonal is assumed to be np.inf.\n",
    "            n_ants (int): Number of ants running per iteration\n",
    "            n_best (int): Number of best ants who deposit pheromone\n",
    "            n_iteration (int): Number of iterations\n",
    "            decay (float): Rate it which pheromone decays. The pheromone value is multiplied by decay, so 0.95 will lead to decay, 0.5 to much faster decay.\n",
    "            alpha (int or float): exponenet on pheromone, higher alpha gives pheromone more weight. Default=1\n",
    "            beta (int or float): exponent on distance, higher beta give distance more weight. Default=1\n",
    "        Example:\n",
    "            ant_colony = AntColony(german_distances, 100, 20, 2000, 0.95, alpha=1, beta=2)          \n",
    "        \"\"\"\n",
    "        self.distances  = distances\n",
    "        self.pheromone = np.ones(self.distances.shape) / len(distances)\n",
    "        self.all_inds = range(len(distances))\n",
    "        self.n_ants = n_ants\n",
    "        self.n_best = n_best\n",
    "        self.n_iterations = n_iterations\n",
    "        self.decay = decay\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.matrix=np.zeros((len(distances),len(distances)))\n",
    "        self.start_points=start_points\n",
    "        \n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        shortest_path = None\n",
    "        all_time_shortest_path = (\"placeholder\", np.inf)\n",
    "        for i in range(self.n_iterations):\n",
    "            all_paths = self.gen_all_paths(i)\n",
    "            self.spread_pheronome(all_paths, self.n_best, shortest_path=shortest_path)\n",
    "            shortest_path = min(all_paths, key=lambda x: x[1])\n",
    "            print (shortest_path)\n",
    "            if shortest_path[1] < all_time_shortest_path[1]:\n",
    "                all_time_shortest_path = shortest_path            \n",
    "            self.pheromone = self.pheromone * self.decay\n",
    "        \n",
    "#         with open('test.txt', 'w') as f:\n",
    "#             f.write(json.dumps(self.matrix.tolist()))\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        return all_time_shortest_path\n",
    "    \n",
    "    \n",
    "  \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    def spread_pheronome(self, all_paths, n_best, shortest_path):\n",
    "        sorted_paths = sorted(all_paths, key=lambda x: x[1])\n",
    "        for path, dist in sorted_paths[:n_best]:\n",
    "            for move in path:\n",
    "                self.pheromone[move] += 1.0 / self.distances[move]\n",
    "\n",
    "    def gen_path_dist(self, path):\n",
    "        total_dist = 0\n",
    "        for ele in path:\n",
    "            total_dist += self.distances[ele]\n",
    "        return total_dist\n",
    "\n",
    "    def gen_all_paths(self,no):\n",
    "        all_paths = []\n",
    "        for i in range(self.n_ants):\n",
    "            #starting=random.randint(0,len(self.distances)-1)\n",
    "            starting=self.start_points[no]\n",
    "            path = self.gen_path(starting)\n",
    "           \n",
    "            all_paths.append((path, self.gen_path_dist(path)))\n",
    "            \n",
    "        return all_paths\n",
    "\n",
    "    def gen_path(self, start):\n",
    "        path = []\n",
    "        visited = set()\n",
    "        visited.add(start)\n",
    "        prev = start\n",
    "        for i in range(len(self.distances) - 1):\n",
    "            move=self.pick_move(self.pheromone[prev], self.distances[prev], visited)\n",
    "            if(move==\"Exit\"):\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "\n",
    "                \n",
    "                path.append((prev, move))\n",
    "                self.matrix[prev,move]=self.matrix[prev,move]+self.distances[prev,move] \n",
    "                prev = move\n",
    "                visited.add(move)\n",
    "        if(move==\"Exit\"):\n",
    "            return path\n",
    "        else:\n",
    "            \n",
    "            path.append((prev, start)) # going back to where we started    \n",
    "            return path\n",
    "\n",
    "    def pick_move(self, pheromone, dist, visited):\n",
    "        pheromone = np.copy(pheromone)\n",
    "        pheromone[list(visited)] = 0\n",
    "        row = pheromone ** self.alpha * (( 1.0 / dist) ** self.beta)\n",
    "        row[row == inf] = 0\n",
    "        row=np.nan_to_num(row, nan=0.0)\n",
    "        norm_row = row / row.sum()\n",
    "        norm_row=np.nan_to_num(norm_row, nan=0.0)\n",
    "\n",
    "       \n",
    "        if(np.all((norm_row == 0))):\n",
    "            return \"Exit\"\n",
    "            \n",
    "           \n",
    "    \n",
    "       \n",
    "        else:\n",
    "            move = np_choice(self.all_inds,1, p=norm_row)[0]\n",
    "            return move\n",
    "        \n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "ant_colony = AntColony(np.array(model.our_tfidf_matrix),1, 1, len(start_points), 0.95,start_points ,alpha=1, beta=1) \n",
    "shortest_path = ant_colony.run()\n",
    "print (\"shorted_path: {}\".format(shortest_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d1f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_colony2 = AntColony(np.array(model2.our_tfidf_matrix),1, 1, len(start_points2), 0.95,start_points2, alpha=1, beta=1,) \n",
    "shortest_path2 = ant_colony2.run()\n",
    "print (\"shorted_path: {}\".format(shortest_path2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb25665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Graph():\n",
    "    \n",
    "    def __init__(self,save_location,graph=None):\n",
    "        self.graph=graph\n",
    "        self.save_location=save_location\n",
    "       \n",
    "    def importMatrix(self,graph_location):\n",
    "        \n",
    "        with open(graph_location, 'r') as f:\n",
    "            self.graph = json.loads(f.read())\n",
    "            \n",
    "        self.graph=np.array(self.graph)\n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def drawGraph(self):\n",
    "        \n",
    "        Z = nx.DiGraph(self.graph)\n",
    "        labels = nx.get_edge_attributes(Z,'weight')\n",
    "        G = nx.DiGraph()                                                                                                                                     \n",
    "        fedges = filter(lambda x: Z.degree()[x[0]] > 0 and Z.degree()[x[1]] > 0,Z.edges())\n",
    "        G.add_edges_from(fedges)\n",
    "        f = plt.figure()\n",
    "        f.set_figwidth(90)\n",
    "        f.set_figheight(90)\n",
    "        pos=nx.spring_layout(G) \n",
    "        nx.draw_networkx(G,pos)\n",
    "\n",
    "        nx.draw_networkx_edge_labels(G,pos,edge_labels=labels)\n",
    "        plt.savefig(self.save_location,bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_instance=Graph(\"our_tfidf_malware_graph1.png\",graph=np.array(ant_colony.matrix))\n",
    "graph_instance.drawGraph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00eac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_instance2=Graph(\"our_tfidf_goodware_graph1.png\",graph=np.array(ant_colony2.matrix))\n",
    "\n",
    "graph_instance2.drawGraph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_instance3=Graph(\"our_word2vec_goodware_graph.png\")\n",
    "# graph_instance3.importMatrix(\"our_results_word2vec_goodware.txt\")\n",
    "# graph_instance3.drawGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_instance4=Graph(\"our_word2vec_malware_graph.png\")\n",
    "# graph_instance4.importMatrix(\"our_results_word2vec_malware.txt\")\n",
    "# graph_instance4.drawGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe51bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
